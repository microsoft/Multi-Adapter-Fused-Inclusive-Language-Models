# Introduction 
Modular Transfer Learning Approaches for Debiasing LLMs :: [MAFIA](https://arxiv.org/abs/2402.07519)

# Getting Started

We recommend creating a virtual environment before installing the package (optional):

```shell
$ {sudo} pip install virtualenv
$ virtualenv -p python3 modraienv
$ source modraienv/bin/activate
```

Install the Inclusivity toolkit provided in the repository. Please refer to `InclusivityToolkit/README.md` for further instructions. 

Note: If you are using an A100 GPU we recommend installing [pytorch](https://pytorch.org/get-started/locally/) first seperately {Optional}:

Install the package requirements
```shell
pip install -r requirements.txt
```

# Running the Code

## Preparing Data

@Prachi Jain, please add more here

Data preparation involves generating counterfactuals and tokenizing the generated data.

Counterfactual augmented data for gender dimension can be generated by running the following command:

```shell
$ python -m src.cda.cda_generate --output_file {path_to_store_cda_data}/{output_filename} --wikipedia_data_dir {wiki_data_dir} 
```
python -m src.cda.cda_generate --output_file  ~/project/data/wikipedia/wiki_cda/religion/raw.txt --wikipedia_data_dir  ~/project/data/wikipedia/20200501.en.hf/ --bias_type religion

Generated data can then be tokenized to avoid repeated tokenization during training by running:

```shell
$ python -m src.data_utils.tokenize_lm_corpus --txt_file {path_to_store_cda_data}/{output_filename} --tokenizer_variant {tokenizer_to_use} --block_size {max_length_of_text}
```
python -m src.data_utils.tokenize_lm_corpus --txt_file ~/project/data/wikipedia/wiki_cda/religion/raw.txt --tokenizer_variant bert-base-uncased --block_size 128

## Train and evaluate the model

Use `src/pretrain_dba.py` to train a DBA for individual bias dimensions. See `scripts/run_dba.sh` for more info.

Each of these debiased adapters can be used for task specific fine-tuning with `src/finetune_on_downstream_task.py`. See `scripts/run_ft.sh` for more info.

Finally, to fuse multiple adapters on a downstream task, use `src/minimal_task_ft.py`. See `scripts/mafia_all_biases.sh` for more info.

# Transparency Note
1.	The paper created a dataset of counterfactual race, religion and gender pairs. The paper also proposes a model to debias language models. The paper also has new evaluation dataset for race and religion bias and multi-lingual bias evaluation. The paper was published at EACL 2024 in March. We release the above artifacts – counterfactual dataset, model, evaluation dataset with the repository.
2.	In this study, we explore several methods for debiasing PLMs and evaluate them on various end tasks and languages. These methods are primarily designed for the English language, they may not perform equally well for all languages of the world.
3.	In this work, we only explore the interplay between a limited set of biases, i.e., gender, race, religion, and profession, and agree that numerous other biases such as cultural and psychological biases have not been addressed. Similarly, we select a limited set of high and low-resource languages for zero-shot evaluation.
4.	Our CF pairs are limited by the knowledge of text-davinici-003 and presence in WikiData. For computaional efficiency, the number of CF pairs are further reduced on the basis of the frequency of the occurrence of the entities in the pair, in Google Book Corpus.
5.	We also acknowledge that our AdapterFusion is tuned on the downstream task, which makes it task-specific and not generic.
6.	We only investigate the effect of fusion on a few downstream tasks, and replicating these findings on other tasks like Bias-NLI would be an interesting study.
7.	Lastly, we were also constrained by our limited computational resources, as “pretraining” the debiasing adapters consumed a significant time for larger models like RoBERTa and XLM-R.
8.	
# Project

> This repo has been populated by an initial template to help get you started. Please
> make sure to update the content to build a great experience for community-building.

As the maintainer of this project, please make a few updates:

- Improving this README.MD file to provide a great experience
- Updating SUPPORT.MD with content about this project's support experience
- Understanding the security reporting process in SECURITY.MD
- Remove this section from the README

## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## Trademarks

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft 
trademarks or logos is subject to and must follow 
[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party's policies.
